{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor(),\n",
    "    download = True,\n",
    ")\n",
    "\n",
    "train_data.data = train_data.data.type(torch.float32)\n",
    "train_data.targets = train_data.targets.type(torch.float32)\n",
    "\n",
    "test_data.data = test_data.data.type(torch.float32)\n",
    "test_data.targets = test_data.targets.type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 7417/10000\n",
      "epoch 1: 9504/10000\n",
      "epoch 2: 9565/10000\n",
      "epoch 3: 9497/10000\n",
      "epoch 4: 9577/10000\n",
      "epoch 5: 9564/10000\n",
      "epoch 6: 9581/10000\n",
      "epoch 7: 9600/10000\n",
      "epoch 8: 9601/10000\n",
      "epoch 9: 9579/10000\n",
      "epoch 10: 9609/10000\n",
      "epoch 11: 9599/10000\n",
      "epoch 12: 9590/10000\n",
      "epoch 13: 9587/10000\n",
      "epoch 14: 9568/10000\n",
      "epoch 15: 9622/10000\n",
      "epoch 16: 9620/10000\n",
      "epoch 17: 9612/10000\n",
      "epoch 18: 9607/10000\n",
      "epoch 19: 9603/10000\n"
     ]
    }
   ],
   "source": [
    "def fix_input(x):\n",
    "    x =  x.flatten().reshape(1, 784)\n",
    "    return x/255.0\n",
    "\n",
    "# train_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=False, num_workers=1)\n",
    "\n",
    "# for idx, (data, target) in enumerate(train_loader):\n",
    "#     print(data[0][0].shape)\n",
    "\n",
    "\n",
    "class CustomDataSet:\n",
    "    def __init__(self, data, targets, transform = None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, target\n",
    "    \n",
    "customData = CustomDataSet(data=train_data.data, targets=train_data.targets, transform=fix_input)\n",
    "train_loader = DataLoader(dataset=customData, batch_size=10, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "\n",
    "    def __init__(self, layers: list[int], lr):\n",
    "        torch.manual_seed(1)\n",
    "        # self.biases = [torch.randn(1, layers[i], dtype=torch.float32, requires_grad=True) for i in range(1, len(layers))]\n",
    "        # self.weights = [torch.randn(layers[i], layers[i+1], dtype=torch.float32, requires_grad=True) for i in range(len(layers)-1)]\n",
    "        self.forward_fns = [nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "\n",
    "        self.weights = [fn.weight for fn in self.forward_fns]\n",
    "        self.biases = [fn.bias for fn in self.forward_fns]\n",
    "\n",
    "        self.lr = lr\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.weights + self.biases, lr = self.lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # for b,w in zip(self.biases, self.weights):\n",
    "        #     x = torch.sigmoid(x@w+b)\n",
    "        # return x\n",
    "        for fn in self.forward_fns:\n",
    "            x = torch.relu(fn(x))\n",
    "        return x\n",
    "\n",
    "    def conv(self, y):\n",
    "        arr = [0]*10\n",
    "        arr[int(y.item())] = 1.0\n",
    "        return torch.tensor([arr])\n",
    "\n",
    "    def predict(self, y):\n",
    "        m = 0\n",
    "        I = 0\n",
    "        for i in range(10):\n",
    "            if y[0][i] > m:\n",
    "                I = i\n",
    "                m = y[0][i]\n",
    "        return I\n",
    "\n",
    "    def train(self, training_loader, testing_data):\n",
    "        for epoch in range(20):\n",
    "            for idx, (data, targets) in enumerate(training_loader):\n",
    "                for i in range(len(data)):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    targets = targets.type(torch.float32)\n",
    "                    y_pred = self.forward(data[i])\n",
    "                    y = self.conv(targets[i])\n",
    "                    l = self.loss(y_pred, y)\n",
    "                    l.backward()\n",
    "                    self.optimizer.step()\n",
    "            correct = 0\n",
    "            for i in range(len(testing_data.data)):\n",
    "                y = self.forward(fix_input(testing_data.data[i]))\n",
    "                z = self.predict(y)\n",
    "                if z == testing_data.targets[i]:\n",
    "                    correct += 1\n",
    "            \n",
    "            print(f\"epoch {epoch}: {correct}/{len(testing_data.data)}\")\n",
    "        \n",
    "\n",
    "\n",
    "n = NeuralNet([784, 30, 10], lr=0.1)\n",
    "\n",
    "\n",
    "n.train(train_loader, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
